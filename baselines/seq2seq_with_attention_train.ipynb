{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_attention_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfbrDQZ-R8cW"
      },
      "source": [
        "**Note:** Initial development was done for a Persian-English dataset and hence some variables have misleading names. As long as you configure setter variables for language and functions correctly, you should be able to train on whichever language you wish to. You can add to the list of languages and provide correct filepaths to reuse this code for a different language pair. The code is currently configured to run for the Ro-En data in our final experiments.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gp5ijm4xeMw"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v73lbJrOxiTo"
      },
      "source": [
        "import torchtext\r\n",
        "torchtext.__version__\r\n",
        "!pip install torchtext==0.6.0\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "!pip install openpyxl\r\n",
        "from openpyxl import load_workbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT0ZMB_tzNq3"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "from os import path\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext import data, datasets\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "from torchtext.data.metrics import bleu_score\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "import time\r\n",
        "from datetime import datetime\r\n",
        "import ast\r\n",
        "import nltk\r\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6izZf0YQzQW1"
      },
      "source": [
        "source_languages = ['fa', 'ro']\r\n",
        "set_language_idx = 1 #set language index from above list\r\n",
        "set_language = source_languages[1] #Set source language\r\n",
        "set_mode = '15'  #Only required if set_language is ro. choose between '15' or 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bBKf_jzlhY"
      },
      "source": [
        "if set_language == 'fa':\r\n",
        "    en_file_path = 'drive/MyDrive/en-fa/mizan_en.txt'\r\n",
        "    fa_file_path = 'drive/MyDrive/en-fa/mizan_fa.txt'\r\n",
        "    data_size = 500 #set to 'full' to load all\r\n",
        "    train_path = 'drive/MyDrive/en-fa/train_mizan_'\r\n",
        "    validation_path = 'drive/MyDrive/en-fa/val_mizan_'\r\n",
        "    test_path = 'drive/MyDrive/en-fa/test_mizan_'\r\n",
        "    base_path = 'drive/MyDrive/en-fa/'\r\n",
        "    weights_path = 'drive/MyDrive/en-fa/weights/'\r\n",
        "\r\n",
        "elif set_language == 'ro':\r\n",
        "    if set_mode == '15':\r\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_en.txt'\r\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_ro.txt'\r\n",
        "    elif set_mode == 'all':\r\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_en.txt'\r\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_ro.txt'     \r\n",
        "    data_size = 500 #set to 'full' to load all\r\n",
        "\r\n",
        "    if set_mode == '15':\r\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_15_'\r\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_15_'\r\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_15_'\r\n",
        "    elif set_mode == 'all':\r\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_all_'\r\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_all_'\r\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_all_'        \r\n",
        "    base_path = 'drive/MyDrive/en-ro/'\r\n",
        "    weights_path = 'drive/MyDrive/en-ro/weights/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgbBO4nvzpjQ"
      },
      "source": [
        "if not path.exists(base_path+'s2s_attention_progress.xlsx'):\r\n",
        "    workbook = xlsxwriter.Workbook(base_path+'s2s_attention_progress.xlsx')\r\n",
        "    worksheet = workbook.add_worksheet()\r\n",
        "    worksheet.write('A1', 'Run_ID') \r\n",
        "    worksheet.write('B1', 'Epoch') \r\n",
        "    worksheet.write('C1', 'Bleu')\r\n",
        "    worksheet.write('D1', 'Mean_loss')\r\n",
        "    worksheet.write('E1', 'Duration')\r\n",
        "    worksheet.write('F1', 'Sample')\r\n",
        "    workbook.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPvsUWisz2Js"
      },
      "source": [
        "#train/val/test splitter (NOT REQUIRED FOR RO)\r\n",
        "#set file paths as required\r\n",
        "if set_language_idx == 0:\r\n",
        "\r\n",
        "    en_sentences = []\r\n",
        "    with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "        for line in f:\r\n",
        "            \r\n",
        "            en_sentences.append(str(line))\r\n",
        "    fa_sentences = []\r\n",
        "    with open(fa_file_path, encoding=\"utf8\") as f:\r\n",
        "        for line in f:\r\n",
        "            fa_sentences.append(str(line))\r\n",
        "            \r\n",
        "    mode = ['train', 'val', 'test']\r\n",
        "    lang = ['en','fa']\r\n",
        "    data_split = [35000, 1000, 2000]                 #set train, val, test sizes\r\n",
        "    sample_indices = random.sample(range(len(en_sentences)), sum(data_split))\r\n",
        "    for language in lang:\r\n",
        "        try:\r\n",
        "            os.remove(train_path + language + '.txt')\r\n",
        "            os.remove(test_path + language + '.txt')\r\n",
        "            os.remove(validation_path + language + '.txt')\r\n",
        "            print(language + ' File delete successful')\r\n",
        "        except OSError:\r\n",
        "            print('no deletion')\r\n",
        "            pass\r\n",
        "        if language == 'en':\r\n",
        "            sentences = en_sentences\r\n",
        "        elif language == 'fa':\r\n",
        "            sentences = fa_sentences\r\n",
        "        \r\n",
        "        for task in mode:\r\n",
        "            if task == 'train':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[:data_split[0]]]\r\n",
        "                #print(len(write_sentences))\r\n",
        "    \r\n",
        "            elif task == 'val':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]:data_split[0]+data_split[1]]]\r\n",
        "            elif task == 'test':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]+data_split[1]:sum(data_split)]]\r\n",
        "            outF = open(base_path + task+'_mizan_'+language + '.txt', 'w', encoding=\"utf8\")\r\n",
        "            base_path + task+'_mizan_'+language + '.txt'\r\n",
        "            for line in write_sentences:\r\n",
        "              # write line to output file\r\n",
        "                outF.write(line)\r\n",
        "                #outF.write(\"\\n\")\r\n",
        "            outF.close()\r\n",
        "\r\n",
        "else:\r\n",
        "    print('not reuired for selected source language')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNbk-ioBz8Tg"
      },
      "source": [
        "#RO VAL SET CREATION _ ONLY NEED TO RUN FIRST TIME\r\n",
        "try:\r\n",
        "    os.remove(validation_path + 'ro.txt')\r\n",
        "    os.remove(validation_path + 'en.txt')\r\n",
        "    print(language + ' File delete successful')\r\n",
        "except OSError:\r\n",
        "    print('no deletion')\r\n",
        "\r\n",
        "en_sentences = []\r\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        \r\n",
        "        en_sentences.append(str(line))\r\n",
        "ro_sentences = []\r\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        ro_sentences.append(str(line))\r\n",
        "val_size = 50 #set size of val file\r\n",
        "en_val = en_sentences[:val_size]\r\n",
        "ro_val = ro_sentences[:val_size]\r\n",
        "for language in ['en', 'ro']:\r\n",
        "    if language == 'en':\r\n",
        "        write_sentences = en_val\r\n",
        "    elif language == 'ro':\r\n",
        "        write_sentences = ro_val\r\n",
        "    outF = open(base_path + 'validating_nevoie_' + set_mode + '_'+ language + '.txt', 'w', encoding=\"utf8\")\r\n",
        "        \r\n",
        "    for line in write_sentences:\r\n",
        "      # write line to output file\r\n",
        "        outF.write(line)\r\n",
        "        #outF.write(\"\\n\")\r\n",
        "    outF.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPV09VMY0GPp"
      },
      "source": [
        "train_len_checker = []\r\n",
        "val_len_checker = []\r\n",
        "test_len_checker = []\r\n",
        "with open(train_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        train_len_checker.append(str(line))\r\n",
        "print(len(train_len_checker))\r\n",
        "with open(validation_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        val_len_checker.append(str(line))\r\n",
        "print(len(val_len_checker))\r\n",
        "with open(test_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        test_len_checker.append(str(line))\r\n",
        "print(len(test_len_checker))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2q7zMpH0Knh"
      },
      "source": [
        "#generate random test case\r\n",
        "en_sentences = []\r\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "    i = 0\r\n",
        "    for line in f:\r\n",
        "        if data_size != 'full':\r\n",
        "            if i==data_size:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                en_sentences.append(str(line))\r\n",
        "        else:\r\n",
        "            en_sentences.append(str(line))\r\n",
        "        i+=1\r\n",
        "en_sentences = [sentence.lower() for sentence in en_sentences]\r\n",
        "\r\n",
        "fa_sentences = []\r\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:  ### CHANGE FILEPATH ACCORDING TO LANGUAGE\r\n",
        "    i = 0\r\n",
        "    for line in f:\r\n",
        "        if data_size != 'full':\r\n",
        "            if i==data_size:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                fa_sentences.append(str(line))\r\n",
        "        else:\r\n",
        "            fa_sentences.append(str(line))\r\n",
        "        i+=1\r\n",
        "\r\n",
        "random_src_sentence = fa_sentences[random.sample(range(50,len(fa_sentences)), 1)[0]].replace('.', '').replace(':', '').replace(',','').replace(';','').replace('!','').replace('\\n', '')\r\n",
        "print(random_src_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHkgipPE0ajo"
      },
      "source": [
        "#tokenizer\r\n",
        "def tokenizer(sentence):\r\n",
        "    sentence = sentence.lower().replace('\\n', '')\r\n",
        "    sentence = sentence.replace('.', '').replace(':', '').replace(',',' ').replace(';','').replace('!','').replace(\"'\", '').replace('  ', ' ').replace('?', '')  #might change\r\n",
        "    tokenized = sentence.split(' ')\r\n",
        "    return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97UZ6j-V0jlb"
      },
      "source": [
        "#build vocab\r\n",
        "farsi = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\r\n",
        "english = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\r\n",
        "source_exts = ['fa.txt', 'ro.txt']\r\n",
        "train_data = datasets.TranslationDataset(\r\n",
        "    path=train_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "validation_data = datasets.TranslationDataset(\r\n",
        "    path=validation_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "test_data = datasets.TranslationDataset(\r\n",
        "    path=test_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "test_data = test_data[50:]\r\n",
        "\r\n",
        "farsi.build_vocab(train_data, max_size = 100000, min_freq = 2)\r\n",
        "english.build_vocab(train_data, max_size = 100000, min_freq = 2)\r\n",
        "train_data[5].__dict__.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx_SEjJm0tpV"
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dq8T5T401js"
      },
      "source": [
        "#test and evaluation\r\n",
        "\r\n",
        "bleu_libs = ['torchtext', 'nltk']\r\n",
        "bleu_idx = 1\r\n",
        "bleu_version = bleu_libs[bleu_idx]   #set bleu choice\r\n",
        "\r\n",
        "def translate_sentence(model, sentence, farsi, english, device, max_length=50):\r\n",
        "    if type(sentence) == str:\r\n",
        "        tokens = tokenizer(sentence)\r\n",
        "    elif type(sentence) == list:\r\n",
        "        tokens = sentence\r\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\r\n",
        "    tokens.insert(0, farsi.init_token)\r\n",
        "    tokens.append(farsi.eos_token)\r\n",
        "\r\n",
        "    # Go through each german token and convert to an index\r\n",
        "    text_to_indices = [farsi.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    # Convert to Tensor\r\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        outputs_encoder, hiddens, cells = model.encoder(sentence_tensor)    \r\n",
        "\r\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\r\n",
        "\r\n",
        "    for _ in range(max_length):\r\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, hiddens, cells = model.decoder(\r\n",
        "                previous_word, outputs_encoder, hiddens, cells\r\n",
        "            )\r\n",
        "            best_guess = output.argmax(1).item()\r\n",
        "\r\n",
        "        outputs.append(best_guess)\r\n",
        "\r\n",
        "        # Model predicts it's the end of the sentence\r\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "\r\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\r\n",
        "\r\n",
        "    # remove start token\r\n",
        "    return translated_sentence[1:]\r\n",
        "\r\n",
        "def bleu(data, model, farsi, english, device):\r\n",
        "    targets = []\r\n",
        "    outputs = []\r\n",
        "\r\n",
        "    for example in data:\r\n",
        "        src = vars(example)[\"src\"]\r\n",
        "        trg = vars(example)[\"trg\"]\r\n",
        "\r\n",
        "        prediction = translate_sentence(model, src, farsi, english, device)\r\n",
        "        prediction = prediction[:-1]  # remove <eos> token\r\n",
        "        \r\n",
        "        if bleu_idx == 0:\r\n",
        "            targets.append([trg])\r\n",
        "        elif bleu_idx == 1:\r\n",
        "            targets.append(trg)\r\n",
        "        outputs.append(prediction)\r\n",
        "    \r\n",
        "    if bleu_idx == 0:\r\n",
        "        return bleu_score(outputs, targets)\r\n",
        "    elif bleu_idx == 1:\r\n",
        "        smoothie = SmoothingFunction().method4\r\n",
        "        return nltk.translate.bleu_score.corpus_bleu(targets, outputs, smoothing_function=smoothie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajsKpvZjrkWJ"
      },
      "source": [
        "def save_checkpoint(state, filename=weights_path+\"s2s_a_checkpoint.pth.tar\"):\r\n",
        "    print(\"=> Saving checkpoint\")\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint, model, optimizer):\r\n",
        "    print(\"=> Loading checkpoint\")\r\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class EncoderAttnRNN(nn.Module):\r\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\r\n",
        "        super(EncoderAttnRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        # dropout wont work for rnn if we have only one layer\r\n",
        "        rnn_dropout = dropout_p if num_layers > 1 else 0.0\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=rnn_dropout, bidirectional=True)\r\n",
        "\r\n",
        "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\r\n",
        "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\r\n",
        "\r\n",
        "\r\n",
        "#         self.dropout = nn.Dropout(p)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # x = [src_length, batch_size]\r\n",
        "\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # embedding shape: (seq_length, batch_size, embedding_size)\r\n",
        "\r\n",
        "        # ------------------------------------------------\r\n",
        "        # -------- DIFFERENCE WITH A BASIC Seq2Seq -------\r\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\r\n",
        "        # encoder_states = [src_length, batch_size, hidden_size*num_directions]\r\n",
        "        # hidden = [n_layers*num_directions, batch_size, hidden_size]\r\n",
        "        # hidden has one forward and one backward\r\n",
        "\r\n",
        "        # dim=2\r\n",
        "        # hidden = [2, batch_size, hidden_size]\r\n",
        "        # Use forward, backward cells and hidden through a linear layer\r\n",
        "        # so that it can be input to the decoder which is not bidirectional\r\n",
        "        # Also using index slicing ([idx:idx+1]) to keep the dimension\r\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\r\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\r\n",
        "        # we also need to send all h_j (hidden states for all time steps) to the decoder\r\n",
        "        # so we also return encoder_states\r\n",
        "        # ------------------------------------------------\r\n",
        "\r\n",
        "        return encoder_states, hidden, cell\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class DecoderAttnRNN(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\r\n",
        "    ):\r\n",
        "        super(DecoderAttnRNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        # dropout wont work for rnn if we have only one layer\r\n",
        "        rnn_dropout = dropout_p if num_layers > 1 else 0.0\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\r\n",
        "        # hidden_size*2, one for forward and one for backward\r\n",
        "        # embedding_size is just the normal one, like in the basic seq2seq model\r\n",
        "\r\n",
        "        self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, num_layers, dropout=rnn_dropout)\r\n",
        "\r\n",
        "        # hidden from encoder and also s_(i-1) which is the previous hidden of decoder\r\n",
        "        # therefore it is *3\r\n",
        "        self.energy = nn.Linear(hidden_size * 3, 1)\r\n",
        "        # also try this later: self.softmax = nn.LogSoftmax(dim=0)\r\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\r\n",
        "        self.dropout = nn.Dropout(p)\r\n",
        "        self.softmax = nn.Softmax(dim=0)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self, x, encoder_states, hidden, cell):\r\n",
        "        # this hidden is from the decoder\r\n",
        "        # and encoder_states are from the encoder\r\n",
        "        # we need to concat them together and then send through energy layer\r\n",
        "\r\n",
        "        # encoder_states = []\r\n",
        "        # x = [batch_size], we want it to be (1, batch_size),\r\n",
        "        # seq_length is 1 here because we are sending in a single word and not a sentence\r\n",
        "        x = x.unsqueeze(0)\r\n",
        "        # x = [1, batch_size]\r\n",
        "\r\n",
        "        embedding = self.dropout(self.embedding(x))\r\n",
        "        # embedding = [1, batch_size, embedding_size]\r\n",
        "\r\n",
        "        # ------------------------------------------------\r\n",
        "        # -------- DIFFERENCE WITH A BASIC Seq2Seq -------\r\n",
        "        sequence_length = encoder_states.shape[0]\r\n",
        "        # in order to add the hidden from decoder to encoder\r\n",
        "        # we repeat the hidden from decoder\r\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\r\n",
        "        # h_reshaped: (seq_length, N, hidden_size*2)\r\n",
        "\r\n",
        "\r\n",
        "        # add them along dim=2, at the end will be hidden_size*3 dimension\r\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\r\n",
        "        attention = self.softmax(energy)\r\n",
        "        # attention = [sequence_length, batch_size, 1]\r\n",
        "        # in self.softmax we said dim=0, so here it will normalize over sequence_length\r\n",
        "\r\n",
        "#         # in order to use bmm for multiplication, we need to permute\r\n",
        "#         attention = attention.permute(1,2,0)\r\n",
        "#         # attention = [batch_size, 1, sequence_length]\r\n",
        "#         encoder_states = encoder_states.permute(1,0,2)\r\n",
        "#         # encoder_states = [batch_size, sequence_length, hidden_size*2]\r\n",
        "#         # now bmm on these two:\r\n",
        "#         # [batch_size, 1, sequence_length] and\r\n",
        "#         # [batch_size, sequence_length, hidden_size*2]\r\n",
        "#         context_vector = torch.bmm(attention, encoder_states)\r\n",
        "#         # context_vector = [batch_size, 1, hidden_size*2]\r\n",
        "#         context_vector = context_vector.permute(1,0,2)\r\n",
        "#         # context_vector = [1, batch_size, hidden_size*2]\r\n",
        "\r\n",
        "        # N is batch_size\r\n",
        "        # attention: (seq_length, N, 1), snk\r\n",
        "        # encoder_states: (seq_length, N, hidden_size*2), snl\r\n",
        "        # we want context_vector: (1, N, hidden_size*2), i.e knl\r\n",
        "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\r\n",
        "\r\n",
        "        # this is for one time step, we need to concat all of them\r\n",
        "        # along hidden_size*2 to get hidden_size*3\r\n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\r\n",
        "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\r\n",
        "        # ------------------------------------------------\r\n",
        "\r\n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\r\n",
        "        # outputs shape = [1, batch_size, hidden_size]\r\n",
        "\r\n",
        "        predictions = self.fc(outputs)\r\n",
        "        # predictions = self.softmax(self.fc(x_out))\r\n",
        "        # predictions = [1, batch_size, length_of_vocab]\r\n",
        "        # but to send it to loss function we want it to be\r\n",
        "        #  (batch_size, length_target_vocab)\r\n",
        "        # so we're just gonna remove the first dim\r\n",
        "        # predictions = [N, hidden_size]\r\n",
        "        predictions = predictions.squeeze(0)\r\n",
        "        # predictions = [batch_size, length_of_vocab]\r\n",
        "\r\n",
        "        return predictions, hidden, cell\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG6yyL0JsGfX"
      },
      "source": [
        "class Seq2SeqAttn(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super(Seq2SeqAttn, self).__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "\r\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\r\n",
        "        batch_size = source.shape[1]\r\n",
        "        target_len = target.shape[0]\r\n",
        "        target_vocab_size = len(english.vocab)\r\n",
        "\r\n",
        "        # prepare hidden states\r\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\r\n",
        "\r\n",
        "        # ------------------------------------------------\r\n",
        "        # -------- DIFFERENCE WITH A BASIC Seq2Seq -------\r\n",
        "        encoder_states, hidden, cell = self.encoder(source)\r\n",
        "        # ------------------------------------------------\r\n",
        "\r\n",
        "        # First input will be <SOS> token\r\n",
        "        x = target[0]\r\n",
        "\r\n",
        "        for t in range(1, target_len):\r\n",
        "\r\n",
        "            # ------------------------------------------------\r\n",
        "            # -------- DIFFERENCE WITH A BASIC Seq2Seq -------\r\n",
        "            # at 'every' time step we will also send the encoder_states to the decoder,\r\n",
        "            # and update hidden, cell\r\n",
        "\r\n",
        "            # Use previous hidden, cell as context from encoder at start\r\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\r\n",
        "            # ------------------------------------------------\r\n",
        "\r\n",
        "            # Store prediction for current time step\r\n",
        "            # Store next output prediction, along the first dimension (target_len)\r\n",
        "            outputs[t] = output\r\n",
        "\r\n",
        "\r\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\r\n",
        "            best_guess = output.argmax(1)\r\n",
        "\r\n",
        "            # With probability of teacher_force_ratio we take the actual next word\r\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\r\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\r\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\r\n",
        "            # then inputs at test time might be completely different than what the\r\n",
        "            # network is used to.\r\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\r\n",
        "\r\n",
        "        return outputs\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0tjAIiUsOSx"
      },
      "source": [
        "# Training\r\n",
        "### We're ready to define everything we need for training our Seq2Seq model ###\r\n",
        "now = datetime.now()\r\n",
        "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\r\n",
        "run_id_dict = {'start_time': dt_string, 'source sentence': random_src_sentence}\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "load_model = True\r\n",
        "model_loaded = False\r\n",
        "save_model = True\r\n",
        "\r\n",
        "# Training hyperparameters\r\n",
        "num_epochs = 100\r\n",
        "learning_rate = 3e-4\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Model hyperparameters\r\n",
        "input_size_encoder = len(farsi.vocab)\r\n",
        "input_size_decoder = len(english.vocab)\r\n",
        "output_size = len(english.vocab)\r\n",
        "encoder_embedding_size = 300\r\n",
        "decoder_embedding_size = 300\r\n",
        "hidden_size = 1024\r\n",
        "num_layers = 1\r\n",
        "# enc_dropout = 0.0\r\n",
        "# dec_dropout = 0.0\r\n",
        "dropout_p = 0.5\r\n",
        "rnn_type = 'lstm'\r\n",
        "\r\n",
        "# Tensorboard to get nice loss plot\r\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\r\n",
        "step = 0\r\n",
        "\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, validation_data, test_data),\r\n",
        "    batch_size=batch_size,\r\n",
        "    sort_within_batch=True,\r\n",
        "    sort_key=lambda x: len(x.src),\r\n",
        "    device=device\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "encoder_net = EncoderAttnRNN(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, dropout_p).to(device)\r\n",
        "decoder_net = DecoderAttnRNN(\r\n",
        "    input_size_decoder,\r\n",
        "    decoder_embedding_size,\r\n",
        "    hidden_size,\r\n",
        "    output_size,\r\n",
        "    num_layers,\r\n",
        "    dropout_p,\r\n",
        ").to(device)\r\n",
        "\r\n",
        "model = Seq2SeqAttn(encoder_net, decoder_net).to(device)\r\n",
        "\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\r\n",
        "\r\n",
        "wb = load_workbook(filename = base_path+'s2s_attention_progress.xlsx')\r\n",
        "ws = wb.active\r\n",
        "\r\n",
        "if load_model:\r\n",
        "\r\n",
        "    if not model_loaded:\r\n",
        "        wb = load_workbook(filename = base_path+'s2s_attention_progress.xlsx')\r\n",
        "        ws = wb.active\r\n",
        "        start_epoch = len(ws[\"A\"])-1\r\n",
        "        run_id_retrieved = ws.cell(row=len(ws[\"A\"]), column = 1).value\r\n",
        "        run_id_retrieved = run_id_retrieved.replace(\"'start_time'\", '\"start_time\"').replace(\"'source sentence'\", '\"source sentence\"')\r\n",
        "        run_id_parsed = ast.literal_eval(run_id_retrieved)\r\n",
        "        sentence = run_id_parsed['source sentence'].rstrip().lstrip()\r\n",
        "\r\n",
        "    if path.exists(weights_path+'s2s_a_checkpoint.pth.tar'):\r\n",
        "        print('weight path exists')\r\n",
        "        if model_loaded == False:\r\n",
        "            load_checkpoint(torch.load(weights_path+'s2s_a_checkpoint.pth.tar'), model, optimizer)\r\n",
        "            print('weight load check')\r\n",
        "            model_loaded = True\r\n",
        "\r\n",
        "if not load_model:\r\n",
        "    start_epoch = 0\r\n",
        "    sentence = random_src_sentence\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator =  BucketIterator.splits((train_data, validation_data, test_data),\r\n",
        "                                                                        batch_size = batch_size,\r\n",
        "                                                                        sort_within_batch = True,\r\n",
        "                                                                        # sort_key: we have all sentences in avarious length, here\r\n",
        "                                                                        # it is priotrize to batch with same length to minimize padding to save on compute\r\n",
        "                                                                        sort_key = lambda x: len(x.src),\r\n",
        "                                                                        device = device)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "for epoch in range(start_epoch, num_epochs):\r\n",
        "\r\n",
        "    start_time = time.time()\r\n",
        "\r\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\r\n",
        "\r\n",
        "    if save_model:\r\n",
        "        checkpoint = {\r\n",
        "            \"state_dict\": model.state_dict(),\r\n",
        "            \"optimizer\": optimizer.state_dict(),\r\n",
        "        }\r\n",
        "        save_checkpoint(checkpoint)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    translated_sentence = translate_sentence(\r\n",
        "        model, sentence, farsi, english, device, max_length=50\r\n",
        "    )\r\n",
        "\r\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    losses = []\r\n",
        "\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "        # Get input and targets and get to cuda\r\n",
        "        inp_data = batch.src.to(device)\r\n",
        "        target = batch.trg.to(device)\r\n",
        "\r\n",
        "        # Forward prop\r\n",
        "        output = model(inp_data, target)\r\n",
        "\r\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\r\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\r\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\r\n",
        "        # way that we have output_words * batch_size that we want to send in into\r\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\r\n",
        "        # Let's also remove the start token while we're at it\r\n",
        "        output = output[1:].reshape(-1, output.shape[2])\r\n",
        "        target = target[1:].reshape(-1)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss = criterion(output, target)\r\n",
        "        \r\n",
        "        losses.append(loss.item())    \r\n",
        "\r\n",
        "        # Back prop\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\r\n",
        "        # within a healthy range\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\r\n",
        "\r\n",
        "        # Gradient descent step\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Plot to tensorboard\r\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\r\n",
        "        step += 1\r\n",
        "\r\n",
        "    mean_loss = sum(losses) / len(losses)\r\n",
        "    score = bleu(test_data, model, farsi, english, device)\r\n",
        "    print(f\"Bleu score {score * 100:.2f}\")\r\n",
        "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "    write_row_idx = len(ws[\"A\"])+1\r\n",
        "    if not load_model:\r\n",
        "        ws.cell(row=write_row_idx,column=1).value = str(run_id_dict)\r\n",
        "    else:\r\n",
        "        ws.cell(row=write_row_idx,column=1).value = run_id_retrieved\r\n",
        "    ws.cell(row=write_row_idx,column=2).value = epoch\r\n",
        "    ws.cell(row=write_row_idx,column=3).value = round(score * 100, 2) #Bleu\r\n",
        "    ws.cell(row=write_row_idx,column=4).value = mean_loss #Mean Loss\r\n",
        "    ws.cell(row=write_row_idx,column=5).value = ((time.time() - start_time)/60) #Duration\r\n",
        "    ws.cell(row=write_row_idx,column=6).value = str(translated_sentence) #sample translation\r\n",
        "    wb.save(base_path+'s2s_attention_progress.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}