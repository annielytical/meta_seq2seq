{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_seq2seq_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iznwg0pWN76Z"
      },
      "source": [
        "**Note:** Initial development was done for a Persian-English dataset and hence some variables have misleading names. As long as you configure setter variables for language and functions correctly, you should be able to train on whichever language you wish to. You can add to the list of languages and provide correct filepaths to reuse this code for a different language pair. The code is currently configured to run for the Ro-En data in our final experiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNlkCkC6zlms"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PYg6Ylm7eKy"
      },
      "source": [
        "import torchtext\n",
        "torchtext.__version__\n",
        "!pip install torchtext==0.6.0\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter\n",
        "!pip install openpyxl\n",
        "from openpyxl import load_workbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc-fMM8y4Ze9"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext import data, datasets\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "import numpy as np\n",
        "import spacy\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import ast\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-NxtDitCCY"
      },
      "source": [
        "source_languages = ['fa', 'ro']\r\n",
        "set_language_idx = 1 #set language index from above list\r\n",
        "set_language = source_languages[1] #Set source language\r\n",
        "set_mode = '15'  #Only required if set_language is ro. choose between '15' or 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqK7RSq4R7gH"
      },
      "source": [
        "**File Paths**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aRp_YOB2BP2"
      },
      "source": [
        "if set_language == 'fa':\n",
        "    en_file_path = 'drive/MyDrive/en-fa/mizan_en.txt'\n",
        "    fa_file_path = 'drive/MyDrive/en-fa/mizan_fa.txt'\n",
        "    data_size = 500 #set to 'full' to load all\n",
        "    train_path = 'drive/MyDrive/en-fa/train_mizan_'\n",
        "    validation_path = 'drive/MyDrive/en-fa/val_mizan_'\n",
        "    test_path = 'drive/MyDrive/en-fa/test_mizan_'\n",
        "    base_path = 'drive/MyDrive/en-fa/'\n",
        "    weights_path = 'drive/MyDrive/en-fa/weights/'\n",
        "\n",
        "elif set_language == 'ro':\n",
        "    if set_mode == '15':\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_en.txt'\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_ro.txt'\n",
        "    elif set_mode == 'all':\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_en.txt'\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_ro.txt'     \n",
        "    data_size = 500 #set to 'full' to load all\n",
        "\n",
        "    if set_mode == '15':\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_15_'\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_15_'\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_15_'\n",
        "    elif set_mode == 'all':\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_all_'\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_all_'\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_all_'        \n",
        "    base_path = 'drive/MyDrive/en-ro/'\n",
        "    weights_path = 'drive/MyDrive/en-ro/weights/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Ve0HgRtQNz"
      },
      "source": [
        "if not path.exists(base_path+'seq2seq_progress.xlsx'):\r\n",
        "    workbook = xlsxwriter.Workbook(base_path+'seq2seq_progress.xlsx')\r\n",
        "    worksheet = workbook.add_worksheet()\r\n",
        "    worksheet.write('A1', 'Run_ID') \r\n",
        "    worksheet.write('B1', 'Epoch') \r\n",
        "    worksheet.write('C1', 'Bleu') \r\n",
        "    worksheet.write('D1', 'Mean loss')\r\n",
        "    worksheet.write('E1', 'Duration')\r\n",
        "    worksheet.write('F1', 'Sample')\r\n",
        "    workbook.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9qdJfL3yuvD"
      },
      "source": [
        "**File creation (run only if required)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo7an0nAy0PZ"
      },
      "source": [
        "#train/val/test splitter (NOT REQUIRED FOR RO)\n",
        "#set file paths as required\n",
        "if set_language_idx == 0:\n",
        "\n",
        "    en_sentences = []\n",
        "    with open(en_file_path, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            \n",
        "            en_sentences.append(str(line))\n",
        "    fa_sentences = []\n",
        "    with open(fa_file_path, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            fa_sentences.append(str(line))\n",
        "            \n",
        "    mode = ['train', 'val', 'test']\n",
        "    lang = ['en','fa']\n",
        "    data_split = [35000, 1000, 2000]                 #set train, val, test sizes\n",
        "    sample_indices = random.sample(range(len(en_sentences)), sum(data_split))\n",
        "    for language in lang:\n",
        "        try:\n",
        "            os.remove(train_path + language + '.txt')\n",
        "            os.remove(test_path + language + '.txt')\n",
        "            os.remove(validation_path + language + '.txt')\n",
        "            print(language + ' File delete successful')\n",
        "        except OSError:\n",
        "            print('no deletion')\n",
        "            pass\n",
        "        if language == 'en':\n",
        "            sentences = en_sentences\n",
        "        elif language == 'fa':\n",
        "            sentences = fa_sentences\n",
        "        \n",
        "        for task in mode:\n",
        "            if task == 'train':\n",
        "                write_sentences = [sentences[i] for i in sample_indices[:data_split[0]]]\n",
        "                #print(len(write_sentences))\n",
        "    \n",
        "            elif task == 'val':\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]:data_split[0]+data_split[1]]]\n",
        "            elif task == 'test':\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]+data_split[1]:sum(data_split)]]\n",
        "            outF = open(base_path + task+'_mizan_'+language + '.txt', 'w', encoding=\"utf8\")\n",
        "            base_path + task+'_mizan_'+language + '.txt'\n",
        "            for line in write_sentences:\n",
        "              # write line to output file\n",
        "                outF.write(line)\n",
        "                #outF.write(\"\\n\")\n",
        "            outF.close()\n",
        "\n",
        "else:\n",
        "    print('not reuired for selected source language')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9UN1IFIuMu9"
      },
      "source": [
        "#RO VAL SET CREATION -- ONLY RUN FIRST TIME\r\n",
        "try:\r\n",
        "    os.remove(validation_path + 'ro.txt')\r\n",
        "    os.remove(validation_path + 'en.txt')\r\n",
        "    print(language + ' File delete successful')\r\n",
        "except OSError:\r\n",
        "    print('no deletion')\r\n",
        "\r\n",
        "en_sentences = []\r\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        \r\n",
        "        en_sentences.append(str(line))\r\n",
        "ro_sentences = []\r\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        ro_sentences.append(str(line))\r\n",
        "val_size = 50 #set size of val file\r\n",
        "en_val = en_sentences[:val_size]\r\n",
        "ro_val = ro_sentences[:val_size]\r\n",
        "for language in ['en', 'ro']:\r\n",
        "    if language == 'en':\r\n",
        "        write_sentences = en_val\r\n",
        "    elif language == 'ro':\r\n",
        "        write_sentences = ro_val\r\n",
        "    outF = open(base_path + 'validating_nevoie_' + set_mode + '_'+ language + '.txt', 'w', encoding=\"utf8\")\r\n",
        "        \r\n",
        "    for line in write_sentences:\r\n",
        "      # write line to output file\r\n",
        "        outF.write(line)\r\n",
        "        #outF.write(\"\\n\")\r\n",
        "    outF.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8smmcUe0nX1"
      },
      "source": [
        "train_len_checker = []\n",
        "with open(train_path+'en.txt', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        train_len_checker.append(str(line))\n",
        "len(train_len_checker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hhFiJtMq9aW"
      },
      "source": [
        "**Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3EabAUDrCj1"
      },
      "source": [
        "#generate random test case\n",
        "en_sentences = []\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\n",
        "    i = 0\n",
        "    for line in f:\n",
        "        if data_size != 'full':\n",
        "            if i==data_size:\n",
        "                break\n",
        "            else:\n",
        "                en_sentences.append(str(line))\n",
        "        else:\n",
        "            en_sentences.append(str(line))\n",
        "        i+=1\n",
        "en_sentences = [sentence.lower() for sentence in en_sentences]\n",
        "\n",
        "fa_sentences = []\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:  ### CHANGE FILEPATH ACCORDING TO LANGUAGE\n",
        "    i = 0\n",
        "    for line in f:\n",
        "        if data_size != 'full':\n",
        "            if i==data_size:\n",
        "                break\n",
        "            else:\n",
        "                fa_sentences.append(str(line))\n",
        "        else:\n",
        "            fa_sentences.append(str(line))\n",
        "        i+=1\n",
        "\n",
        "random_src_sentence = fa_sentences[random.sample(range(len(fa_sentences)), 1)[0]].replace('.', '').replace(':', '').replace(',','').replace(';','').replace('!','').replace('\\n', '')\n",
        "print(random_src_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b78CN7HU0ScZ"
      },
      "source": [
        "**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-O0ecvr0YnF"
      },
      "source": [
        "def tokenizer(sentence):\n",
        "    sentence = sentence.lower().replace('\\n', '')\n",
        "    sentence = sentence.replace('.', '').replace(':', '').replace(',',' ').replace(';','').replace('!','').replace(\"'\", '').replace('  ', ' ').replace('?', '')  #might change\n",
        "    tokenized = sentence.split(' ')\n",
        "    return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtN6Fu9HedD0"
      },
      "source": [
        "**Build Train, Validation and Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL-uaBqKeb_G"
      },
      "source": [
        "farsi = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "english = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "source_exts = ['fa.txt', 'ro.txt']\n",
        "train_data = datasets.TranslationDataset(\n",
        "    path=train_path, exts=(source_exts[set_language_idx], 'en.txt'),\n",
        "    fields=(farsi, english))\n",
        "validation_data = datasets.TranslationDataset(\n",
        "    path=validation_path, exts=(source_exts[set_language_idx], 'en.txt'),\n",
        "    fields=(farsi, english))\n",
        "test_data = datasets.TranslationDataset(\n",
        "    path=test_path, exts=(source_exts[set_language_idx], 'en.txt'),\n",
        "    fields=(farsi, english))\n",
        "test_data = test_data[50:]\n",
        "\n",
        "farsi.build_vocab(train_data, max_size = 100000, min_freq = 2)\n",
        "english.build_vocab(train_data, max_size = 100000, min_freq = 2)\n",
        "train_data[5].__dict__.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7GA9-2suxx0"
      },
      "source": [
        "**Testing and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41RvYeUPu2xv"
      },
      "source": [
        "bleu_libs = ['torchtext', 'nltk']\n",
        "bleu_idx = 1\n",
        "bleu_version = bleu_libs[bleu_idx]   #set bleu choice\n",
        "\n",
        "\n",
        "def translate_sentence(model, sentence, farsi, english, device, max_length=50):\n",
        "\n",
        "    #sentence = sentence.replace('\\n', '').replace('.', '').replace(':', '').replace(',','').replace(';','').replace('!','')  #might change\n",
        "    #tokens = sentence.split(' ')\n",
        "    if type(sentence) == str:\n",
        "        tokens = tokenizer(sentence)\n",
        "    elif type(sentence) == list:\n",
        "        tokens = sentence\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, farsi.init_token)\n",
        "    tokens.append(farsi.eos_token)\n",
        "\n",
        "    # Go through each farsi token and convert to an index\n",
        "    text_to_indices = [farsi.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "\n",
        "    # remove start and end tokens\n",
        "    return translated_sentence[1:-1]\n",
        "\n",
        "\n",
        "def bleu(data, model, farsi, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, farsi, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "        \n",
        "        if bleu_idx == 0:\n",
        "            targets.append([trg])\n",
        "        elif bleu_idx == 1:\n",
        "            targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "    \n",
        "    if bleu_idx == 0:\n",
        "        return bleu_score(outputs, targets)\n",
        "    elif bleu_idx == 1:\n",
        "        smoothie = SmoothingFunction().method4\n",
        "        return nltk.translate.bleu_score.corpus_bleu(targets, outputs, smoothing_function=smoothie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AYncBLzTxTl"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePNUCKN8T2XV"
      },
      "source": [
        "def save_checkpoint(state, filename=weights_path+\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, rnn_type, num_layers, dropout_p):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        # dropout wont work for rnn if we have only one layer\n",
        "        rnn_dropout = dropout_p if num_layers > 1 else 0.0\n",
        "\n",
        "        if rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(emb_size, hidden_size, num_layers=n_layers, dropout=rnn_dropout)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=rnn_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [src_length, batch_size]\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding = [src_length, batch_size, embedding_size]\n",
        "\n",
        "        x_out, (hidden, cell) = self.rnn(embedding)\n",
        "        # x_out = [src_length, batch_size, hidden_size*num_directions]\n",
        "        # hidden = [n_layers*num_directions, batch_size, hidden_size]\n",
        "\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout_p):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        # dropout wont work for rnn if we have only one layer\n",
        "        rnn_dropout = dropout_p if num_layers > 1 else 0.0\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=rnn_dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "#         self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x = [batch_size], we want it to be (1, batch_size),\n",
        "        # seq_length is 1 here because we are sending in a single word and not a sentence\n",
        "        x = x.unsqueeze(0)\n",
        "        # x = [1, batch_size]\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding = [1, batch_size, embedding_size]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        # outputs = [1, batch_size, hidden_size]\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "        # predictions = self.softmax(self.fc(x_out))\n",
        "        # predictions = [1, batch_size, length_of_vocab]\n",
        "        # but to send it to loss function we want it to be\n",
        "        #  (batch_size, length_target_vocab)\n",
        "        # so we're just gonna remove the first dim\n",
        "        predictions = predictions.squeeze(0)\n",
        "        # predictions = [batch_size, length_of_vocab]\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(seq2seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = len(english.vocab)\n",
        "\n",
        "        # prepare hidden states\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            # output = [batch_size, target_vocab_size]\n",
        "\n",
        "            # Store next output prediction, along the first dimension (target_len)\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYLJ4P7CKO1t"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-a1ta38S5Mj"
      },
      "source": [
        "# Training\n",
        "\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "run_id_dict = {'start_time': dt_string, 'source sentence': random_src_sentence}\n",
        "\n",
        "# training hyperparams\n",
        "num_epochs = 100\n",
        "learning_r = 0.0001\n",
        "batch_size= 32\n",
        "\n",
        "# model\n",
        "load_model = False\n",
        "model_loaded = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size_encoder = len(farsi.vocab)\n",
        "input_size_decoder = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "enc_embedding_size = 300\n",
        "dec_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "dropout_p = 0.5\n",
        "rnn_type = 'lstm'\n",
        "\n",
        "teacher_force_ratio=0.5\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(f'runs/loss_plot')\n",
        "step = 0\n",
        "\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator =  BucketIterator.splits((train_data, validation_data, test_data),\n",
        "                                                                        batch_size = batch_size,\n",
        "                                                                        sort_within_batch = True,\n",
        "                                                                        # sort_key: we have all sentences in various length, here\n",
        "                                                                        # it is priotrize to batch with same length to minimize padding to save on compute\n",
        "                                                                        sort_key = lambda x: len(x.src),\n",
        "                                                                        device = device)\n",
        "\n",
        "\n",
        "encoder = EncoderRNN(input_size_encoder, enc_embedding_size, hidden_size, rnn_type, num_layers, dropout_p).to(device)\n",
        "decoder = DecoderRNN(input_size_decoder, dec_embedding_size, hidden_size, output_size, num_layers, dropout_p).to(device)\n",
        "\n",
        "model = seq2seq(encoder, decoder).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_r)\n",
        "\n",
        "\n",
        "pad_idx = english.vocab.stoi['<pad>'] # stoi stands for string to index\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "wb = load_workbook(filename = base_path+'seq2seq_progress.xlsx')\n",
        "ws = wb.active\n",
        "\n",
        "if load_model:\n",
        "\n",
        "    if not model_loaded:\n",
        "        wb = load_workbook(filename = base_path+'seq2seq_progress.xlsx')\n",
        "        ws = wb.active\n",
        "        start_epoch = len(ws[\"A\"])-1\n",
        "        run_id_retrieved = ws.cell(row=len(ws[\"A\"]), column = 1).value\n",
        "        run_id_retrieved = run_id_retrieved.replace(\"'start_time'\", '\"start_time\"').replace(\"'source sentence'\", '\"source sentence\"')\n",
        "        run_id_parsed = ast.literal_eval(run_id_retrieved)\n",
        "        sentence = run_id_parsed['source sentence'].rstrip().lstrip()\n",
        "\n",
        "    if path.exists(weights_path+'my_checkpoint.pth.tar'):\n",
        "        if model_loaded == False:\n",
        "            load_checkpoint(torch.load(weights_path+'my_checkpoint.pth.tar'), model, optimizer)\n",
        "            print('weight load check')\n",
        "            model_loaded = True\n",
        "\n",
        "\n",
        "if not load_model:\n",
        "    start_epoch = 0\n",
        "    sentence = random_src_sentence\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
        "\n",
        "    checkpoint = {'state_dict':model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    translated_sentence = translate_sentence(\n",
        "        model, sentence, farsi, english, device, max_length=50\n",
        "    )\n",
        "\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "        # # Forward prop, send these to the model\n",
        "        output = model(inp_data, target, teacher_force_ratio)\n",
        "        # outout = [trgt_length, batch_size, output_dim]\n",
        "\n",
        "        # we can concatenate the first two dim, keep the dim=2 and then put everything else together\n",
        "        # this is because of the crossentropy optimizer which needs a 2D (batch_size, dim)\n",
        "        # the first output is the start token and we won't send that to the model (output[1:])\n",
        "        # and same for the target\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Plot to tensorboard\n",
        "        writer.add_scalar('Traning loss', loss, global_step=step)\n",
        "\n",
        "        step+=1\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    score = bleu(test_data, model, farsi, english, device)\n",
        "    print(f\"Bleu score {score*100:.2f}\")\n",
        "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
        "\n",
        "    write_row_idx = len(ws[\"A\"])+1\n",
        "    if not load_model:\n",
        "        ws.cell(row=write_row_idx,column=1).value = str(run_id_dict)\n",
        "    else:\n",
        "        ws.cell(row=write_row_idx,column=1).value = run_id_retrieved\n",
        "    ws.cell(row=write_row_idx,column=2).value = epoch\n",
        "    ws.cell(row=write_row_idx,column=3).value = round(score * 100, 2) #Bleu\n",
        "    ws.cell(row=write_row_idx,column=4).value = mean_loss #Mean Loss\n",
        "    ws.cell(row=write_row_idx,column=5).value = ((time.time() - start_time)/60) #Duration\n",
        "    ws.cell(row=write_row_idx,column=6).value = str(translated_sentence) #sample translation\n",
        "    wb.save(base_path+'seq2seq_progress.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}