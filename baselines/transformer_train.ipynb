{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuSlU3NdTMqg"
      },
      "source": [
        "**Note:** Initial development was done for a Persian-English dataset and hence some variables have misleading names. As long as you configure setter variables for language and functions correctly, you should be able to train on whichever language you wish to. You can add to the list of languages and provide correct filepaths to reuse this code for a different language pair. The code is currently configured to run for the Ro-En data in our final experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oytVBnTNY9MZ"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vTrOmRtZEwn"
      },
      "source": [
        "import torchtext\r\n",
        "torchtext.__version__\r\n",
        "!pip install torchtext==0.6.0\r\n",
        "!pip install xlsxwriter\r\n",
        "import xlsxwriter\r\n",
        "!pip install openpyxl\r\n",
        "from openpyxl import load_workbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJjuSc7KZGvl"
      },
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "from os import path\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext import data, datasets\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "from torchtext.data.metrics import bleu_score\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import spacy\r\n",
        "import random\r\n",
        "import time\r\n",
        "from datetime import datetime\r\n",
        "import ast\r\n",
        "import nltk\r\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8qXe2Pk2YVJ"
      },
      "source": [
        "source_languages = ['fa', 'ro']\r\n",
        "set_language_idx = 1 #set language index from above list\r\n",
        "set_language = source_languages[1] #Set source language\r\n",
        "set_mode = '15'  #Only required if set_language is ro. choose between '15' or 'all'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXvfq3A5ZJdl"
      },
      "source": [
        "if set_language == 'fa':\r\n",
        "    en_file_path = 'drive/MyDrive/en-fa/mizan_en.txt'\r\n",
        "    fa_file_path = 'drive/MyDrive/en-fa/mizan_fa.txt'\r\n",
        "    data_size = 500 #set to 'full' to load all\r\n",
        "    train_path = 'drive/MyDrive/en-fa/train_mizan_'\r\n",
        "    validation_path = 'drive/MyDrive/en-fa/val_mizan_'\r\n",
        "    test_path = 'drive/MyDrive/en-fa/test_mizan_'\r\n",
        "    base_path = 'drive/MyDrive/en-fa/'\r\n",
        "    weights_path = 'drive/MyDrive/en-fa/weights/'\r\n",
        "\r\n",
        "elif set_language == 'ro':\r\n",
        "    if set_mode == '15':\r\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_en.txt'\r\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_15_ro.txt'\r\n",
        "    elif set_mode == 'all':\r\n",
        "        en_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_en.txt'\r\n",
        "        ro_file_path = 'drive/MyDrive/en-ro/testing_nevoie_all_ro.txt'     \r\n",
        "    data_size = 500 #set to 'full' to load all\r\n",
        "\r\n",
        "    if set_mode == '15':\r\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_15_'\r\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_15_'\r\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_15_'\r\n",
        "    elif set_mode == 'all':\r\n",
        "        train_path = 'drive/MyDrive/en-ro/training_nevoie_all_'\r\n",
        "        validation_path = 'drive/MyDrive/en-ro/validating_nevoie_all_'\r\n",
        "        test_path = 'drive/MyDrive/en-ro/testing_nevoie_all_'        \r\n",
        "    base_path = 'drive/MyDrive/en-ro/'\r\n",
        "    weights_path = 'drive/MyDrive/en-ro/weights/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk4zhhvIXlqx"
      },
      "source": [
        "if not path.exists(base_path+'transformer_progress.xlsx'):\r\n",
        "    workbook = xlsxwriter.Workbook(base_path+'transformer_progress.xlsx')\r\n",
        "    worksheet = workbook.add_worksheet()\r\n",
        "    worksheet.write('A1', 'Run_ID') \r\n",
        "    worksheet.write('B1', 'Epoch') \r\n",
        "    worksheet.write('C1', 'Bleu')\r\n",
        "    worksheet.write('D1', 'Mean_loss')\r\n",
        "    worksheet.write('E1', 'Duration')\r\n",
        "    worksheet.write('F1', 'Sample')\r\n",
        "    workbook.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raMX7mLsZe31"
      },
      "source": [
        "#train/val/test splitter (NOT REQUIRED FOR RO)\r\n",
        "#set file paths as required\r\n",
        "if set_language_idx == 0:\r\n",
        "\r\n",
        "    en_sentences = []\r\n",
        "    with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "        for line in f:\r\n",
        "            \r\n",
        "            en_sentences.append(str(line))\r\n",
        "    fa_sentences = []\r\n",
        "    with open(fa_file_path, encoding=\"utf8\") as f:\r\n",
        "        for line in f:\r\n",
        "            fa_sentences.append(str(line))\r\n",
        "            \r\n",
        "    mode = ['train', 'val', 'test']\r\n",
        "    lang = ['en','fa']\r\n",
        "    data_split = [35000, 1000, 2000]                 #set train, val, test sizes\r\n",
        "    sample_indices = random.sample(range(len(en_sentences)), sum(data_split))\r\n",
        "    for language in lang:\r\n",
        "        try:\r\n",
        "            os.remove(train_path + language + '.txt')\r\n",
        "            os.remove(test_path + language + '.txt')\r\n",
        "            os.remove(validation_path + language + '.txt')\r\n",
        "            print(language + ' File delete successful')\r\n",
        "        except OSError:\r\n",
        "            print('no deletion')\r\n",
        "            pass\r\n",
        "        if language == 'en':\r\n",
        "            sentences = en_sentences\r\n",
        "        elif language == 'fa':\r\n",
        "            sentences = fa_sentences\r\n",
        "        \r\n",
        "        for task in mode:\r\n",
        "            if task == 'train':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[:data_split[0]]]\r\n",
        "                #print(len(write_sentences))\r\n",
        "    \r\n",
        "            elif task == 'val':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]:data_split[0]+data_split[1]]]\r\n",
        "            elif task == 'test':\r\n",
        "                write_sentences = [sentences[i] for i in sample_indices[data_split[0]+data_split[1]:sum(data_split)]]\r\n",
        "            outF = open(base_path + task+'_mizan_'+language + '.txt', 'w', encoding=\"utf8\")\r\n",
        "            base_path + task+'_mizan_'+language + '.txt'\r\n",
        "            for line in write_sentences:\r\n",
        "              # write line to output file\r\n",
        "                outF.write(line)\r\n",
        "                #outF.write(\"\\n\")\r\n",
        "            outF.close()\r\n",
        "\r\n",
        "else:\r\n",
        "    print('not reuired for selected source language')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTetb_BQ7OFV"
      },
      "source": [
        "#RO VAL SET CREATION _ ONLY NEED TO RUN FIRST TIME\r\n",
        "try:\r\n",
        "    os.remove(validation_path + 'ro.txt')\r\n",
        "    os.remove(validation_path + 'en.txt')\r\n",
        "    print(language + ' File delete successful')\r\n",
        "except OSError:\r\n",
        "    print('no deletion')\r\n",
        "\r\n",
        "en_sentences = []\r\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        \r\n",
        "        en_sentences.append(str(line))\r\n",
        "ro_sentences = []\r\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        ro_sentences.append(str(line))\r\n",
        "val_size = 50 #set size of val file\r\n",
        "en_val = en_sentences[:val_size]\r\n",
        "ro_val = ro_sentences[:val_size]\r\n",
        "for language in ['en', 'ro']:\r\n",
        "    if language == 'en':\r\n",
        "        write_sentences = en_val\r\n",
        "    elif language == 'ro':\r\n",
        "        write_sentences = ro_val\r\n",
        "    outF = open(base_path + 'validating_nevoie_' + set_mode + '_'+ language + '.txt', 'w', encoding=\"utf8\")\r\n",
        "        \r\n",
        "    for line in write_sentences:\r\n",
        "      # write line to output file\r\n",
        "        outF.write(line)\r\n",
        "        #outF.write(\"\\n\")\r\n",
        "    outF.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW8Lp1KUZh2H"
      },
      "source": [
        "train_len_checker = []\r\n",
        "val_len_checker = []\r\n",
        "test_len_checker = []\r\n",
        "with open(train_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        train_len_checker.append(str(line))\r\n",
        "print(len(train_len_checker))\r\n",
        "with open(validation_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        val_len_checker.append(str(line))\r\n",
        "print(len(val_len_checker))\r\n",
        "with open(test_path+'en.txt', encoding=\"utf8\") as f:\r\n",
        "    for line in f:\r\n",
        "        test_len_checker.append(str(line))\r\n",
        "print(len(test_len_checker))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_jQcphaZioP"
      },
      "source": [
        "#generate random test case\r\n",
        "en_sentences = []\r\n",
        "with open(en_file_path, encoding=\"utf8\") as f:\r\n",
        "    i = 0\r\n",
        "    for line in f:\r\n",
        "        if data_size != 'full':\r\n",
        "            if i==data_size:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                en_sentences.append(str(line))\r\n",
        "        else:\r\n",
        "            en_sentences.append(str(line))\r\n",
        "        i+=1\r\n",
        "en_sentences = [sentence.lower() for sentence in en_sentences]\r\n",
        "\r\n",
        "fa_sentences = []\r\n",
        "with open(ro_file_path, encoding=\"utf8\") as f:  ### CHANGE FILEPATH ACCORDING TO LANGUAGE\r\n",
        "    i = 0\r\n",
        "    for line in f:\r\n",
        "        if data_size != 'full':\r\n",
        "            if i==data_size:\r\n",
        "                break\r\n",
        "            else:\r\n",
        "                fa_sentences.append(str(line))\r\n",
        "        else:\r\n",
        "            fa_sentences.append(str(line))\r\n",
        "        i+=1\r\n",
        "\r\n",
        "random_src_sentence = fa_sentences[random.sample(range(50,len(fa_sentences)), 1)[0]].replace('.', '').replace(':', '').replace(',','').replace(';','').replace('!','').replace('\\n', '')\r\n",
        "print(random_src_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s1ITA6pZn88"
      },
      "source": [
        "#tokenizer\r\n",
        "def tokenizer(sentence):\r\n",
        "    sentence = sentence.lower().replace('\\n', '')\r\n",
        "    sentence = sentence.replace('.', '').replace(':', '').replace(',',' ').replace(';','').replace('!','').replace(\"'\", '').replace('  ', ' ').replace('?', '')  #might change\r\n",
        "    tokenized = sentence.split(' ')\r\n",
        "    return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kCyar17ZxsA"
      },
      "source": [
        "#build vocab\r\n",
        "farsi = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\r\n",
        "english = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\r\n",
        "source_exts = ['fa.txt', 'ro.txt']\r\n",
        "train_data = datasets.TranslationDataset(\r\n",
        "    path=train_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "validation_data = datasets.TranslationDataset(\r\n",
        "    path=validation_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "test_data = datasets.TranslationDataset(\r\n",
        "    path=test_path, exts=(source_exts[set_language_idx], 'en.txt'),\r\n",
        "    fields=(farsi, english))\r\n",
        "test_data = test_data[50:]\r\n",
        "\r\n",
        "farsi.build_vocab(train_data, max_size = 100000, min_freq = 2)\r\n",
        "english.build_vocab(train_data, max_size = 100000, min_freq = 2)\r\n",
        "train_data[5].__dict__.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZbBrVoqYxr9"
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osngadvvZ7p0"
      },
      "source": [
        "#test and evaluation\r\n",
        "\r\n",
        "bleu_libs = ['torchtext', 'nltk']\r\n",
        "bleu_idx = 1\r\n",
        "bleu_version = bleu_libs[bleu_idx]   #set bleu choice\r\n",
        "\r\n",
        "def translate_sentence(model, sentence, farsi, english, device, max_length=50):\r\n",
        "    if type(sentence) == str:\r\n",
        "        tokens = tokenizer(sentence)\r\n",
        "    elif type(sentence) == list:\r\n",
        "        tokens = sentence\r\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\r\n",
        "    tokens.insert(0, farsi.init_token)\r\n",
        "    tokens.append(farsi.eos_token)\r\n",
        "\r\n",
        "    # Go through each german token and convert to an index\r\n",
        "    text_to_indices = [farsi.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    # Convert to Tensor\r\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\r\n",
        "    for i in range(max_length):\r\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output = model(sentence_tensor, trg_tensor)\r\n",
        "\r\n",
        "        best_guess = output.argmax(2)[-1, :].item()\r\n",
        "        outputs.append(best_guess)\r\n",
        "\r\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\r\n",
        "            break\r\n",
        "\r\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\r\n",
        "    # remove start token\r\n",
        "    return translated_sentence[1:]\r\n",
        "\r\n",
        "\r\n",
        "def bleu(data, model, farsi, english, device):\r\n",
        "    targets = []\r\n",
        "    outputs = []\r\n",
        "\r\n",
        "    for example in data:\r\n",
        "        src = vars(example)[\"src\"]\r\n",
        "        trg = vars(example)[\"trg\"]\r\n",
        "\r\n",
        "        prediction = translate_sentence(model, src, farsi, english, device)\r\n",
        "        prediction = prediction[:-1]  # remove <eos> token\r\n",
        "        \r\n",
        "        if bleu_idx == 0:\r\n",
        "            targets.append([trg])\r\n",
        "        elif bleu_idx == 1:\r\n",
        "            targets.append(trg)\r\n",
        "        outputs.append(prediction)\r\n",
        "    \r\n",
        "    if bleu_idx == 0:\r\n",
        "        return bleu_score(outputs, targets)\r\n",
        "    elif bleu_idx == 1:\r\n",
        "        smoothie = SmoothingFunction().method4\r\n",
        "        return nltk.translate.bleu_score.corpus_bleu(targets, outputs, smoothing_function=smoothie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKlMQwWrZ8kh"
      },
      "source": [
        "#model and checkpoints\r\n",
        "def save_checkpoint(state, filename=weights_path+\"tf_checkpoint.pth.tar\"):\r\n",
        "    print(\"=> Saving checkpoint\")\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint, model, optimizer):\r\n",
        "    print(\"=> Loading checkpoint\")\r\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\r\n",
        "\r\n",
        "class Transformer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        embedding_size,\r\n",
        "        src_vocab_size,\r\n",
        "        trg_vocab_size,\r\n",
        "        src_pad_idx,\r\n",
        "        num_heads,\r\n",
        "        num_encoder_layers,\r\n",
        "        num_decoder_layers,\r\n",
        "        forward_expansion,\r\n",
        "        dropout,\r\n",
        "        max_len,\r\n",
        "        device,\r\n",
        "    ):\r\n",
        "        super(Transformer, self).__init__()\r\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\r\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\r\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\r\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        self.transformer = nn.Transformer(\r\n",
        "            embedding_size,\r\n",
        "            num_heads,\r\n",
        "            num_encoder_layers,\r\n",
        "            num_decoder_layers,\r\n",
        "            forward_expansion,\r\n",
        "            dropout,\r\n",
        "        )\r\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "\r\n",
        "    def make_src_mask(self, src):\r\n",
        "        # src = [src_len, batch_size]\r\n",
        "        # transformer in pytorch need for src_key_padding_mas (N, S) which N is for batch_size\r\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\r\n",
        "        # src_mask = [batch_size, src_len]\r\n",
        "\r\n",
        "        return src_mask.to(self.device)\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        src_seq_length, N = src.shape\r\n",
        "        trg_seq_length, N = trg.shape\r\n",
        "\r\n",
        "        # expand it so that we have it for every sample we send in\r\n",
        "        src_positions = (\r\n",
        "            torch.arange(0, src_seq_length)\r\n",
        "            .unsqueeze(1)\r\n",
        "            .expand(src_seq_length, N)\r\n",
        "            .to(self.device)\r\n",
        "        )\r\n",
        "\r\n",
        "        trg_positions = (\r\n",
        "            torch.arange(0, trg_seq_length)\r\n",
        "            .unsqueeze(1)\r\n",
        "            .expand(trg_seq_length, N)\r\n",
        "            .to(self.device)\r\n",
        "        )\r\n",
        "\r\n",
        "        embed_src = self.dropout(\r\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\r\n",
        "        )\r\n",
        "        embed_trg = self.dropout(\r\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\r\n",
        "        )\r\n",
        "\r\n",
        "        src_padding_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\r\n",
        "            self.device\r\n",
        "        )\r\n",
        "\r\n",
        "        out = self.transformer(\r\n",
        "            embed_src,\r\n",
        "            embed_trg,\r\n",
        "            src_key_padding_mask=src_padding_mask,\r\n",
        "            tgt_mask=trg_mask,\r\n",
        "        )\r\n",
        "        out = self.fc_out(out)\r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfyWmsBDaoLT"
      },
      "source": [
        "#Training\r\n",
        "# We're ready to define everything we need for training our Seq2Seq model\r\n",
        "now = datetime.now()\r\n",
        "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\r\n",
        "run_id_dict = {'start_time': dt_string, 'source sentence': random_src_sentence}\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "load_model = False\r\n",
        "model_loaded = False\r\n",
        "save_model = True\r\n",
        "\r\n",
        "# Training hyperparameters\r\n",
        "num_epochs = 100 #10000\r\n",
        "learning_rate = 3e-4\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Model hyperparameters\r\n",
        "src_vocab_size = len(farsi.vocab)\r\n",
        "trg_vocab_size = len(english.vocab)\r\n",
        "embedding_size = 512\r\n",
        "num_heads = 8\r\n",
        "num_encoder_layers = 3\r\n",
        "num_decoder_layers = 3\r\n",
        "dropout_p = 0.10\r\n",
        "max_len = 300 #100\r\n",
        "forward_expansion = 4\r\n",
        "src_pad_idx = english.vocab.stoi[\"<pad>\"]\r\n",
        "\r\n",
        "# Tensorboard to get nice loss plot\r\n",
        "writer = SummaryWriter(\"runs/loss_plot\")\r\n",
        "step = 0\r\n",
        "\r\n",
        "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, validation_data, test_data),\r\n",
        "    batch_size=batch_size,\r\n",
        "    sort_within_batch=True,\r\n",
        "    sort_key=lambda x: len(x.src),\r\n",
        "    device=device\r\n",
        ")\r\n",
        "\r\n",
        "model = Transformer(\r\n",
        "    embedding_size,\r\n",
        "    src_vocab_size,\r\n",
        "    trg_vocab_size,\r\n",
        "    src_pad_idx,\r\n",
        "    num_heads,\r\n",
        "    num_encoder_layers,\r\n",
        "    num_decoder_layers,\r\n",
        "    forward_expansion,\r\n",
        "    dropout_p,\r\n",
        "    max_len,\r\n",
        "    device,\r\n",
        ").to(device)\r\n",
        "\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n",
        "    optimizer, factor=0.1, patience=10, verbose=True\r\n",
        ")\r\n",
        "\r\n",
        "# ignoring pad index in loss, since they arent relevant for computing the loss\r\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\r\n",
        "\r\n",
        "wb = load_workbook(filename = base_path+'transformer_progress.xlsx')\r\n",
        "ws = wb.active\r\n",
        "\r\n",
        "if load_model:\r\n",
        "\r\n",
        "    if not model_loaded:\r\n",
        "        wb = load_workbook(filename = base_path+'transformer_progress.xlsx')\r\n",
        "        ws = wb.active\r\n",
        "        start_epoch = len(ws[\"A\"])-1\r\n",
        "        run_id_retrieved = ws.cell(row=len(ws[\"A\"]), column = 1).value\r\n",
        "        run_id_retrieved = run_id_retrieved.replace(\"'start_time'\", '\"start_time\"').replace(\"'source sentence'\", '\"source sentence\"')\r\n",
        "        run_id_parsed = ast.literal_eval(run_id_retrieved)\r\n",
        "        sentence = run_id_parsed['source sentence'].rstrip().lstrip()\r\n",
        "\r\n",
        "    if path.exists(weights_path+'tf_checkpoint.pth.tar'):\r\n",
        "        if model_loaded == False:\r\n",
        "            load_checkpoint(torch.load(weights_path+'tf_checkpoint.pth.tar'), model, optimizer)\r\n",
        "            print('weight load check')\r\n",
        "            model_loaded = True\r\n",
        "\r\n",
        "if not load_model:\r\n",
        "    start_epoch = 0\r\n",
        "    sentence = random_src_sentence\r\n",
        "\r\n",
        "for epoch in range(start_epoch,num_epochs):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "\r\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\r\n",
        "\r\n",
        "    if save_model:\r\n",
        "        checkpoint = {\r\n",
        "            \"state_dict\": model.state_dict(),\r\n",
        "            \"optimizer\": optimizer.state_dict(),\r\n",
        "        }\r\n",
        "        save_checkpoint(checkpoint)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    translated_sentence = translate_sentence(\r\n",
        "        model, sentence, farsi, english, device, max_length=50\r\n",
        "    )\r\n",
        "\r\n",
        "    print(f\"Translated example sentence: \\n {translated_sentence}\")\r\n",
        "    model.train()\r\n",
        "    losses = []\r\n",
        "\r\n",
        "    for batch_idx, batch in enumerate(train_iterator):\r\n",
        "\r\n",
        "        \r\n",
        "        # Get input and targets and get to cuda\r\n",
        "        inp_data = batch.src.to(device)\r\n",
        "        target = batch.trg.to(device)\r\n",
        "\r\n",
        "        # Forward prop\r\n",
        "        output = model(inp_data, target[:-1, :])\r\n",
        "\r\n",
        "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\r\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\r\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\r\n",
        "        # way that we have output_words * batch_size that we want to send in into\r\n",
        "        # our cost function, so we need to do some reshapin.\r\n",
        "        # Let's also remove the start token while we're at it\r\n",
        "        output = output.reshape(-1, output.shape[2])\r\n",
        "        target = target[1:].reshape(-1)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        loss = criterion(output, target)\r\n",
        "        losses.append(loss.item())\r\n",
        "\r\n",
        "        # Back prop\r\n",
        "        loss.backward()\r\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\r\n",
        "        # within a healthy range\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\r\n",
        "\r\n",
        "        # Gradient descent step\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # plot to tensorboard\r\n",
        "        writer.add_scalar(\"Training loss\", loss, global_step=step)\r\n",
        "        step += 1\r\n",
        "\r\n",
        "    mean_loss = sum(losses) / len(losses)\r\n",
        "    scheduler.step(mean_loss)\r\n",
        "\r\n",
        "    # running on entire test data takes a while\r\n",
        "    score = bleu(test_data, model, farsi, english, device)\r\n",
        "    print(f\"Bleu score {score * 100:.2f}\")\r\n",
        "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\r\n",
        "\r\n",
        "    write_row_idx = len(ws[\"A\"])+1\r\n",
        "    if not load_model:\r\n",
        "        ws.cell(row=write_row_idx,column=1).value = str(run_id_dict)\r\n",
        "    else:\r\n",
        "        ws.cell(row=write_row_idx,column=1).value = run_id_retrieved\r\n",
        "    ws.cell(row=write_row_idx,column=2).value = epoch\r\n",
        "    ws.cell(row=write_row_idx,column=3).value = round(score * 100, 2) #Bleu\r\n",
        "    ws.cell(row=write_row_idx,column=4).value = mean_loss #Mean Loss\r\n",
        "    ws.cell(row=write_row_idx,column=5).value = ((time.time() - start_time)/60) #Duration\r\n",
        "    ws.cell(row=write_row_idx,column=6).value = str(translated_sentence) #sample translation\r\n",
        "    wb.save(base_path+'transformer_progress.xlsx')\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}